{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb01698a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.10.14' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import opendatasets as od \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "# text preprocessing \n",
    "import re \n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import warnings\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae69018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# od.download('https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58573f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataset as pandas dataframe    \n",
    "df = pd.read_csv('imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce42de23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312d3712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of dataset \n",
    "print(\"Shape of the dataset:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a3a6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ac7193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates \n",
    "print(\"Number of duplicate rows:\", df.duplicated().sum()    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97b0b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate \n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05436e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of dataset \n",
    "print(\"New shape of the dataset after dropping duplicates:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb96fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the distribution of target variable\n",
    "print(df.sentiment.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3b5a27",
   "metadata": {},
   "source": [
    "### The data seems to be balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81b1f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation and special characters and emojis\n",
    "def remove_punct(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)      # remove digits\n",
    "    text = re.sub(r'\\s+', ' ', text)     # remove extra spaces\n",
    "    text = re.sub(r'\\n', ' ', text)      # remove new line characters\n",
    "    text = re.sub(r'\\t', ' ', text)      # remove tab characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)  # remove non-ASCII characters\n",
    "    return text.strip()                  # remove leading/trailing spaces\n",
    "\n",
    "# remove stopwords\n",
    "def remove_stpwrd(text):\n",
    "    stpWrd = stopwords.words('english')\n",
    "    return [word for word in text if word not in stpWrd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3220ac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7f8448",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].str.lower()\n",
    "df['review'] = df['review'].apply(remove_punct)\n",
    "df['review'] = df['review'].apply(word_tokenize)\n",
    "df['review'] = df['review'].apply(remove_stpwrd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2623c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115224a5",
   "metadata": {},
   "source": [
    "In the context of sentiment classification, both stemming and lemmatization can be helpful, but they address different needs. Stemming is faster and simpler, reducing words to their root form, while lemmatization considers the word's context and part of speech to produce a meaningful base form (lemma). For sentiment classification, lemmatization is generally preferred if you need more accurate results and can tolerate the slightly slower processing time. However, if speed is critical and some inaccuracies are acceptable, stemming might be a better option. \n",
    "\n",
    "**Stemming**:\n",
    "- Process: A process of reducing words to their root form by removing suffixes (e.g., \"running\" becomes \"run\").\n",
    "    - Pros: Faster and simpler to implement. \n",
    "    - Cons: Can produce non-words (e.g., \"studies\" might become \"studi\"). May not always result in meaningful base forms. \n",
    "\n",
    "**Lemmatization**:\n",
    "- Process: Reduces words to their dictionary form (lemma), considering the word's part of speech (e.g., \"better\" becomes \"good\").\n",
    "    - Pros: Produces more accurate and meaningful base forms.\n",
    "    - Cons: More computationally expensive and slower than stemming. \n",
    "\n",
    "## Sentiment Analysis:\n",
    "### Why lemmatization might be better:\n",
    "\n",
    "- In sentiment analysi, you want to accurately identify and group related words. Lemmatization helps ensure that variations of a word are treated as the same (e.g., \"activate\" and \"activated\" are both reduced to \"activate\"). This can improve the model's ability to recognize positive or negative phrases and patterns. \n",
    "\n",
    "### When stemming might be sufficient:\n",
    "- If your priority is speed and you are dealing with a large dataset where the differences between stemming and lemmatization might not be significant, stemming could be a good starting point. However, if you are seeing issues with accuracy, switching to lemmatization might be beneficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e744d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets go for lemmatization \n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemma = WordNetLemmatizer()\n",
    "    return [lemma.lemmatize(word) for word in text]\n",
    "\n",
    "df['review'] = df.review.apply(lemmatize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a198e61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[100,:][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b5bd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# message length distribution, with labels\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "sns.histplot(data=df.iloc[:1000,:], x=df.review.apply(len), hue='sentiment', kde=True, bins=30)\n",
    "plt.title('Review Length Distribution by Label')\n",
    "plt.xlabel('Review Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42849b50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
